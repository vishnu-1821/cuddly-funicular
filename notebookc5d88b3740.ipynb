{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3637312,"sourceType":"datasetVersion","datasetId":2157551}],"dockerImageVersionId":30474,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-13T09:52:54.917698Z","iopub.execute_input":"2023-05-13T09:52:54.918114Z","iopub.status.idle":"2023-05-13T09:52:54.931111Z","shell.execute_reply.started":"2023-05-13T09:52:54.918082Z","shell.execute_reply":"2023-05-13T09:52:54.92959Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/body-signal-of-smoking/smoking.csv')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:54.93333Z","iopub.execute_input":"2023-05-13T09:52:54.933782Z","iopub.status.idle":"2023-05-13T09:52:55.263687Z","shell.execute_reply.started":"2023-05-13T09:52:54.933735Z","shell.execute_reply":"2023-05-13T09:52:55.262367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data'dan gördüğümüz kadarıyla smoking columnunu 1 ve 0 lardan oluşuyor. Yine de testimizi yapalım.\n.nuniqiue fonksiyonu ile sadece 2 farklı değeri olduğunu görüyoruz","metadata":{}},{"cell_type":"code","source":"df.smoking.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.265368Z","iopub.execute_input":"2023-05-13T09:52:55.266007Z","iopub.status.idle":"2023-05-13T09:52:55.273473Z","shell.execute_reply.started":"2023-05-13T09:52:55.265964Z","shell.execute_reply":"2023-05-13T09:52:55.272384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bu datamızdaki amacımız sağlık değerlerine sigara kullanımının etkisi var mı ? Yok mu ?\nBu sebeple datamız içindeki yalnızca kullanacağımız futureları alıyoruz.","metadata":{}},{"cell_type":"code","source":"dfs = df.iloc[:,10:27]\ndfs.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.275792Z","iopub.execute_input":"2023-05-13T09:52:55.276147Z","iopub.status.idle":"2023-05-13T09:52:55.312412Z","shell.execute_reply.started":"2023-05-13T09:52:55.276115Z","shell.execute_reply":"2023-05-13T09:52:55.311365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ve bu datamız içinde de yine kullanmayacağımız bölümler var. Drop ile kullanmadığımız futureları atabilirdim\nfakat sıralı bi şekilde durdukları için .iloc daha kolay geldi ve kalan 3 future da drop yoluyla çıkardım.","metadata":{}},{"cell_type":"code","source":"dfs = dfs.drop(['oral','tartar','Urine protein'],axis=1)\ndfs.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.313574Z","iopub.execute_input":"2023-05-13T09:52:55.313944Z","iopub.status.idle":"2023-05-13T09:52:55.343003Z","shell.execute_reply.started":"2023-05-13T09:52:55.313877Z","shell.execute_reply":"2023-05-13T09:52:55.341817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Şimdi predict edeceğimiz 'smoking' columnunu y değişkenine. Predict etmek için kullanacağımız veriyi ise predict\nedeceğimiz future olmadan x_data değişkenine atıyoruz.","metadata":{}},{"cell_type":"code","source":"y = dfs.smoking.values\nx_data = dfs.drop(['smoking'],axis=1)\n\nprint(y.shape,x_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.344685Z","iopub.execute_input":"2023-05-13T09:52:55.345956Z","iopub.status.idle":"2023-05-13T09:52:55.356763Z","shell.execute_reply.started":"2023-05-13T09:52:55.345911Z","shell.execute_reply":"2023-05-13T09:52:55.355165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ayırdığımız futurelar içinde çok yüksek ve düşük değerler birlikte bulunuyor. Bunu engellemek için normalize etmemiz\ngerekiyor. Yani kısaca futurlarımızı 0 ile 1 arasında değerlere eşliyoruz.","metadata":{}},{"cell_type":"code","source":"x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.358713Z","iopub.execute_input":"2023-05-13T09:52:55.359102Z","iopub.status.idle":"2023-05-13T09:52:55.38995Z","shell.execute_reply.started":"2023-05-13T09:52:55.359067Z","shell.execute_reply":"2023-05-13T09:52:55.388692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Datamızın bir bölümünü modelimizi train etmek için bir bölümünü ise test için kullanacağız. Sklearn kütüphanesi ile\nbu ayrımı yüzde 35'e 65 olarak ayarladım. Bu modelimizin hata payı en düşük olan değerdi deneyerek seçildi.(test_size)\nRandom state'i ise bu seçilen kısmın random değil her çalıştırıldığında aynı kısım olmasıni istetiğimizden 42 verdik\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.35,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.391244Z","iopub.execute_input":"2023-05-13T09:52:55.391606Z","iopub.status.idle":"2023-05-13T09:52:55.415331Z","shell.execute_reply.started":"2023-05-13T09:52:55.391573Z","shell.execute_reply":"2023-05-13T09:52:55.414154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lojistik regresyon modeli formül;\nFuturelarımızı deneyerek belirlediğimiz bir değer(weight) ile çarpıyoruz ve ardından bias ile topluyoruz\nElde ettiğimiz sonucu z değişkenine atıyoruz. z değişkenini sigmoid fonksiyona sokuyoruz ve sonucumuz\nbize tahmin etmek istediğimiz binary değeri veriyor. (0.8)>0.5 so it is 1","metadata":{}},{"cell_type":"markdown","source":"z = (w.t)x + b","metadata":{}},{"cell_type":"markdown","source":"Weightimizi ve biasımızı belirliyoruz","metadata":{}},{"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.418723Z","iopub.execute_input":"2023-05-13T09:52:55.419113Z","iopub.status.idle":"2023-05-13T09:52:55.424651Z","shell.execute_reply.started":"2023-05-13T09:52:55.419078Z","shell.execute_reply":"2023-05-13T09:52:55.423389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"z değişkenimizi sigmoid fonksiyona sokuyoruz","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    y_head = 1/(1 + np.exp(-z))\n    return y_head","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.426045Z","iopub.execute_input":"2023-05-13T09:52:55.426486Z","iopub.status.idle":"2023-05-13T09:52:55.437668Z","shell.execute_reply.started":"2023-05-13T09:52:55.426444Z","shell.execute_reply":"2023-05-13T09:52:55.436325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train* np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]\n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.439599Z","iopub.execute_input":"2023-05-13T09:52:55.439941Z","iopub.status.idle":"2023-05-13T09:52:55.451434Z","shell.execute_reply.started":"2023-05-13T09:52:55.43991Z","shell.execute_reply":"2023-05-13T09:52:55.450224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.452956Z","iopub.execute_input":"2023-05-13T09:52:55.453798Z","iopub.status.idle":"2023-05-13T09:52:55.469269Z","shell.execute_reply.started":"2023-05-13T09:52:55.453744Z","shell.execute_reply":"2023-05-13T09:52:55.468201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.470653Z","iopub.execute_input":"2023-05-13T09:52:55.471559Z","iopub.status.idle":"2023-05-13T09:52:55.483607Z","shell.execute_reply.started":"2023-05-13T09:52:55.471514Z","shell.execute_reply":"2023-05-13T09:52:55.482269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 5, num_iterations = 850)  ","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:52:55.485375Z","iopub.execute_input":"2023-05-13T09:52:55.486113Z","iopub.status.idle":"2023-05-13T09:53:00.021073Z","shell.execute_reply.started":"2023-05-13T09:52:55.48607Z","shell.execute_reply":"2023-05-13T09:53:00.019625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","metadata":{"execution":{"iopub.status.busy":"2023-05-13T09:53:00.023377Z","iopub.execute_input":"2023-05-13T09:53:00.024361Z","iopub.status.idle":"2023-05-13T09:53:00.440458Z","shell.execute_reply.started":"2023-05-13T09:53:00.024275Z","shell.execute_reply":"2023-05-13T09:53:00.439044Z"},"trusted":true},"execution_count":null,"outputs":[]}]}